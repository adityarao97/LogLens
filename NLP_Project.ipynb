{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjjEnvVGUjfX",
        "outputId": "808856e3-7599-41a9-88a2-aa4e3331250c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q fastapi uvicorn sqlalchemy pymysql huggingface_hub serpapi python-dotenv nest_asyncio pyngrok google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBzXeyk7Uwn0",
        "outputId": "659f627b-a55e-4f1f-fd6c-34054c665ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your HF_TOKEN: ··········\n",
            "Enter your SERPAPI_API_KEY: ··········\n",
            "Enter your RDS DB_HOST: mydb-instance.c36ok0iyywfv.us-west-1.rds.amazonaws.com\n",
            "Enter your RDS DB_USER: admin\n",
            "Enter your RDS DB_PASSWORD: ··········\n",
            "Enter your RDS DB_NAME (default log_db): log_db\n",
            "Enter your ngrok auth token2w7RzfN8r38uSdwjlpNTmVULQsx_7ZR7heaTjXiPDaBzJhE5C\n"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "\n",
        "# Prompt for secrets\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your HF_TOKEN: \")\n",
        "os.environ[\"SERPAPI_API_KEY\"] = getpass.getpass(\"Enter your SERPAPI_API_KEY: \")\n",
        "\n",
        "# Prompt for RDS credentials\n",
        "os.environ[\"DB_HOST\"]     = input(\"Enter your RDS DB_HOST: \")\n",
        "os.environ[\"DB_USER\"]     = input(\"Enter your RDS DB_USER: \")\n",
        "os.environ[\"DB_PASSWORD\"] = getpass.getpass(\"Enter your RDS DB_PASSWORD: \")\n",
        "os.environ[\"DB_NAME\"]     = input(\"Enter your RDS DB_NAME (default log_db): \") or \"log_db\"\n",
        "os.environ[\"ngrok_auth_token\"] = input(\"Enter your ngrok auth token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "116OzY9NUygq"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > Combined1.py << 'EOF'\n",
        "import os\n",
        "import re\n",
        "from typing import Any, List, Literal, Optional\n",
        "from functools import lru_cache\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from serpapi import GoogleSearch\n",
        "from huggingface_hub import InferenceClient\n",
        "from sqlalchemy import create_engine, text\n",
        "from sqlalchemy.exc import SQLAlchemyError\n",
        "from datetime import datetime\n",
        "\n",
        "HF_TOKEN        = os.getenv(\"HF_TOKEN\")\n",
        "SERPAPI_API_KEY = os.getenv(\"SERPAPI_API_KEY\")\n",
        "DB_HOST         = os.getenv(\"DB_HOST\")\n",
        "DB_USER         = os.getenv(\"DB_USER\")\n",
        "DB_PASSWORD     = os.getenv(\"DB_PASSWORD\")\n",
        "DB_NAME         = os.getenv(\"DB_NAME\")\n",
        "\n",
        "cohere_client     = InferenceClient(provider=\"cohere\",     api_key=HF_TOKEN)\n",
        "hyperbolic_client = InferenceClient(provider=\"hyperbolic\", api_key=HF_TOKEN)\n",
        "search_client     = GoogleSearch({\"engine\":\"google\", \"api_key\":SERPAPI_API_KEY})\n",
        "engine            = create_engine(f\"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\")\n",
        "\n",
        "SQL_SYSTEM_PROMPT = (\n",
        "    \"You are an expert SQL generator. Given this schema:\\\\n\"\n",
        "    \"  logs(id, log_file, log_timestamp, log_level,\\\\n\"\n",
        "    \"       log_module, log_request_id, log_user_id,\\\\n\"\n",
        "    \"       log_ip_address, log_request_method, log_request_url,\\\\n\"\n",
        "    \"       log_protocol, log_status, log_length, log_time, message)\\\\n\"\n",
        "    \"Translate the following user request into exactly one valid MySQL query. Output only the SQL statement.\"\n",
        "    \" If the user asks for anomalous or error logs, automatically filter by log_level IN ('WARNING','ERROR','CRITICAL').\"\n",
        ")\n",
        "SQL_FEW_SHOTS = [\n",
        "    {\"role\":\"user\",\"content\":\"List ERROR logs for glance-api from 2025-04-01 to 2025-04-07.\"},\n",
        "    {\"role\":\"assistant\",\"content\":\"SELECT * FROM logs WHERE log_module='glance-api' AND log_level='ERROR' AND log_timestamp BETWEEN '2025-04-01' AND '2025-04-07';\"},\n",
        "    {\"role\":\"user\",\"content\":\"Count GET requests to '/servers/detail'.\"},\n",
        "    {\"role\":\"assistant\",\"content\":\"SELECT COUNT(*) FROM logs WHERE log_request_method='GET' AND log_request_url LIKE '/servers/detail%';\"}\n",
        "]\n",
        "INJECTION_PATTERNS = [r\"ignore all previous instructions\",r\"system settings\",r\"api[_\\\\- ]?key\",r\"token\",r\"internal settings\"]\n",
        "def is_prompt_injection(text: str)->bool:\n",
        "    t=text.lower(); return any(re.search(p,t) for p in INJECTION_PATTERNS)\n",
        "\n",
        "def get_llm_client(model_name:str)->InferenceClient:\n",
        "    return hyperbolic_client if model_name.startswith(\"Qwen/\") else cohere_client\n",
        "\n",
        "def fetch_recent_logs(limit:int=5)->str:\n",
        "    with engine.connect() as c:\n",
        "        rows=c.execute(text(\n",
        "            \"SELECT log_file, log_level, log_time, message \"\n",
        "            \"FROM logs ORDER BY log_time DESC LIMIT :n\"\n",
        "        ),{\"n\":limit}).fetchall()\n",
        "    return \"### RECENT LOG EXAMPLES:\\\\n\" + \"\\\\n\".join(f\"{r.log_file}|{r.log_level}|{r.log_time}|{r.message}\" for r in rows)\n",
        "\n",
        "def search_web(query:str,num_results:int=5)->List[Any]:\n",
        "    if is_prompt_injection(query): return []\n",
        "    params={\"q\":query,\"engine\":\"google\",\"api_key\":SERPAPI_API_KEY,\"num\":num_results}\n",
        "    return search_client.get_dict(params).get(\"organic_results\",[])\n",
        "\n",
        "@lru_cache(maxsize=128)\n",
        "def sql_from_nl(nl_query:str,model_name:str)->Any:\n",
        "    if is_prompt_injection(nl_query): return {\"sql\":\"\",\"rows\":[]}\n",
        "    llm=get_llm_client(model_name)\n",
        "    msgs=[{\"role\":\"system\",\"content\":SQL_SYSTEM_PROMPT},*SQL_FEW_SHOTS,\n",
        "          {\"role\":\"system\",\"content\":fetch_recent_logs()},{\"role\":\"user\",\"content\":nl_query}]\n",
        "    resp=llm.chat.completions.create(model=model_name,messages=msgs, max_tokens=256, temperature=0.0)\n",
        "    raw=resp.choices[0].message.content.strip()\n",
        "    cleaned=re.sub(r\"^```(?:sql)?\\s*\",\"\",raw); cleaned=re.sub(r\"\\s*```$\",\"\",cleaned).strip()\n",
        "    cleaned=re.sub(r\"\\blog_message\\b\",\"message\",cleaned)\n",
        "    return {\"sql\":cleaned,\"rows\":[]}\n",
        "\n",
        "@lru_cache(maxsize=128)\n",
        "def ask_general(nl_query:str,model_name:str)->Any:\n",
        "    if is_prompt_injection(nl_query): return {\"advice\":\"Prompt injection detected: advice generation refused.\"}\n",
        "    llm=get_llm_client(model_name)\n",
        "    system_msg={\"role\":\"system\",\"content\":\"You are a site-reliability expert. Think step by step then provide a numbered list of recommendations.\"}\n",
        "    user_msg={\"role\":\"user\",\"content\":nl_query}\n",
        "    resp=llm.chat.completions.create(model=model_name,messages=[system_msg,user_msg],max_tokens=256,temperature=0.7)\n",
        "    return {\"advice\":resp.choices[0].message.content.strip()}\n",
        "\n",
        "class AssistantRequest(BaseModel):\n",
        "    mode: Literal[\"search\",\"sql\",\"advice\"]\n",
        "    query:str\n",
        "    model:Optional[str]=\"CohereLabs/c4ai-command-a-03-2025\"\n",
        "\n",
        "class AssistantResponse(BaseModel):\n",
        "    mode:str\n",
        "    data:Any\n",
        "\n",
        "app=FastAPI()\n",
        "app.add_middleware(CORSMiddleware,allow_origins=[\"*\"],allow_credentials=True,allow_methods=[\"*\"],allow_headers=[\"*\"])\n",
        "\n",
        "@app.post(\"/assistant\", response_model=AssistantResponse)\n",
        "async def assistant(req:AssistantRequest):\n",
        "    if req.mode==\"search\": data=search_web(req.query)\n",
        "    elif req.mode==\"sql\":\n",
        "        out=sql_from_nl(req.query,req.model); sql,out_rows=out[\"sql\"],out[\"rows\"]\n",
        "        if sql:\n",
        "            try: out_rows=[list(r) for r in engine.connect().execute(text(sql)).fetchall()]\n",
        "            except SQLAlchemyError: out_rows=[]\n",
        "        data={\"sql\":sql,\"rows\":out_rows}\n",
        "    elif req.mode==\"advice\": data=ask_general(req.query,req.model)\n",
        "    else: data=None\n",
        "    return {\"mode\":req.mode,\"data\":data}\n",
        "EOF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "M07SbDgJVRRC",
        "outputId": "a71fd703-a386-42fb-b8b5-75aa09cacc59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔗 Your FastAPI is live at: NgrokTunnel: \"https://1af3-34-141-184-242.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m3740\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     174.160.40.182:0 - \"\u001b[1mPOST /assistant HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Expose port 8000 via ngrok and run the server in-process\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "ngrok.set_auth_token(os.getenv(\"ngrok_auth_token\"))\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"🔗 Your FastAPI is live at:\", public_url)\n",
        "\n",
        "# Start uvicorn (will block—stop with the square “stop” button in Colab)\n",
        "!uvicorn Combined1:app --host 0.0.0.0 --port 8000"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}